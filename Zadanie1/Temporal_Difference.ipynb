{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pendulum\n",
    "## The notebook describes a solution of the problem of controlling a pendulum to be raised to a vertical position and held in this state using a Temporal-difference Q-learning algorithm "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c4b761593cf217b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Workspace\n",
    "Workspace is divided into 4 values, the cartesian coordinate system x and y, theta angle in radians and tau the torque of the pendulum\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42865de12fab9d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Action Space\n",
    "Force applied to the pendulum in the range of continuous values from -2.0 to 2.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbbc08dd370035b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation Space\n",
    "Observation Space contains 3 values, the x and y coordinates, and the angular velocity of the pendulum.\n",
    "x = cos(theta) continuous value from -1.0 to 1.0\n",
    "y = sin(theta) continuous value from -1.0 to 1.0\n",
    "Angular velocity is continuous value from -8.0 to 8.0\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19272ae10206cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I've made those changes:\n",
    "Converted observation space from 3 values to 2, by obtaining theta angle from its cosine and sine using the x and y values I know, and also I made a discretization from continuous values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37c23aa4b8777653"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I used the built-in award function which is described as:\n",
    "### r = -(theta^2^ + 0.1 * theta_dt^2^ + 0.001 * torque^2^)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc0bc6071c6966b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## My solution is written based on this pseudocode\n",
    "![Pseudo code](./q-learning.jpg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6f0d9696bae920"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c88423ae199b5d8a"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-06T19:28:12.385423400Z",
     "start_time": "2024-04-06T19:28:12.374452100Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "PRELOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# env = gym.make('Pendulum-v1', render_mode=\"human\", max_episode_steps=600); env.metadata['render_fps'] = 60\n",
    "env = gym.make('Pendulum-v1', max_episode_steps=600)\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.95\n",
    "\n",
    "if not PRELOAD:\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.999\n",
    "else:\n",
    "    epsilon = 0.15\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.999\n",
    "\n",
    "episodes = 150000\n",
    "total_reward = 0\n",
    "\n",
    "action_space_size = 41\n",
    "\"\"\" count of action [-2 : 2] with 0.1 step, 0->19 = [-2 : -0.1], 20 = 0, 21->40 = [0.1 : 2] \"\"\"\n",
    "observation_space_size = [63, 161]\n",
    "\"\"\"\n",
    "First is count of pendulum state [-pi : pi] with 0.1 step, 0->31 = [-pi : -pi/31], 32 = 0, 33->62 = [pi/32 : pi]\n",
    "Second is count of pendulum angular velocity [-8 : 8] with 0.1 step, 0->79 = [-8 : -0.1], 80 = 0, 81->160 = [0.1 : 8]\n",
    "\"\"\"\n",
    "\n",
    "total_reward_for_episode = list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:29:29.789689900Z",
     "start_time": "2024-04-06T17:29:29.746762700Z"
    }
   },
   "id": "dd446cf7ea08a44"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "action_space = np.linspace(-2, 2, num=action_space_size)\n",
    "observation_space = [np.linspace(-np.pi, np.pi, num=observation_space_size[0]),\n",
    "                     np.linspace(-8.0, 8.0, num=observation_space_size[1])]\n",
    "\n",
    "if not PRELOAD:\n",
    "    q_table = np.random.uniform(low=-2, high=-0, size=(observation_space_size + [action_space_size]))\n",
    "else:\n",
    "    q_table = np.load(\"D:/Programming/MachineLearning2/Zadanie1/TD_q_table_pendulum.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:29:29.814664700Z",
     "start_time": "2024-04-06T17:29:29.757633200Z"
    }
   },
   "id": "4f451234b4704bca"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def get_discrete_action(state):\n",
    "    theta = np.arctan2(state[0], state[1])\n",
    "    index_state = np.digitize(theta, observation_space[0]) - 1\n",
    "    index_velocity = np.digitize(state[2], observation_space[1]) - 1\n",
    "    action = q_table[index_state, index_velocity]\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T19:28:25.460307Z",
     "start_time": "2024-04-06T19:28:25.439358600Z"
    }
   },
   "id": "61a130fe43543558"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def get_indexes(state):\n",
    "    theta = np.arctan2(state[0], state[1])\n",
    "    index_state = np.digitize(theta, observation_space[0]) - 1\n",
    "    index_velocity = np.digitize(state[2], observation_space[1]) - 1\n",
    "    return [index_state, index_velocity]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T19:28:25.664852200Z",
     "start_time": "2024-04-06T19:28:25.650860800Z"
    }
   },
   "id": "1ae9ab81cfc0178"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Total reward = -243.1905086026395\n",
      "Average reward = -243.1905086026395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 15\u001B[0m\n\u001B[0;32m     11\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, action_space_size)\n\u001B[0;32m     13\u001B[0m step \u001B[38;5;241m=\u001B[39m [action_space[action]]\n\u001B[1;32m---> 15\u001B[0m observation, reward, done, truncated, \u001B[38;5;241m*\u001B[39minfo \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m total_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[0;32m     18\u001B[0m episode_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "File \u001B[1;32mD:\\Programming\\MachineLearning2\\env\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     55\u001B[0m \n\u001B[0;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[1;32mD:\\Programming\\MachineLearning2\\env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Programming\\MachineLearning2\\env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Programming\\MachineLearning2\\env\\lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:143\u001B[0m, in \u001B[0;36mPendulumEnv.step\u001B[1;34m(self, u)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([newth, newthdot])\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 143\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_obs(), \u001B[38;5;241m-\u001B[39mcosts, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "File \u001B[1;32mD:\\Programming\\MachineLearning2\\env\\lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:259\u001B[0m, in \u001B[0;36mPendulumEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    258\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    260\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mflip()\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# mode == \"rgb_array\":\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(get_discrete_action(state))\n",
    "        else:\n",
    "            action = np.random.randint(0, action_space_size)\n",
    "\n",
    "        step = [action_space[action]]\n",
    "\n",
    "        observation, reward, done, truncated, *info = env.step(step)\n",
    "\n",
    "        total_reward += reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Change q table\n",
    "        index_state, index_velocity = get_indexes(state)\n",
    "        new_index_state, new_index_velocity = get_indexes(observation)\n",
    "        new_action = np.argmax(get_discrete_action(observation))\n",
    "\n",
    "        current_q = q_table[index_state, index_velocity, action]\n",
    "        new_step = np.max(q_table[new_index_state, new_index_velocity])\n",
    "        new_q = current_q + learning_rate * (reward + discount_rate * new_step - current_q)\n",
    "        q_table[index_state, index_velocity, action] = new_q\n",
    "        state = observation\n",
    "\n",
    "        # Change epsilon\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        # Truncated\n",
    "        if truncated:\n",
    "            state = env.reset()\n",
    "            break\n",
    "\n",
    "    # if episode % 1000 == 0:\n",
    "    print(\"Episode: {}\".format(episode))\n",
    "    print(\"Total reward = {}\".format(total_reward))\n",
    "    print(\"Average reward = {}\".format(total_reward/episode))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:29:48.956119300Z",
     "start_time": "2024-04-06T17:29:29.782144400Z"
    }
   },
   "id": "d85e52b2be501d0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(total_reward_for_episode)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-06T17:29:48.952306800Z"
    }
   },
   "id": "f645bf177b942669"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.save(\"D:/Programming/MachineLearning2/Zadanie1/q_table_pendulum\", q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-06T17:29:48.954840500Z"
    }
   },
   "id": "5a1dd85cb150f91b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "11443dd5c1b98ee2"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ce73bc528b415a41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d863b9f38d38f9"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d4a5127ad4011a18"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b7ca5a2b77bde85c"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "Total reward = -15763026.519013282\n",
      "Average reward = -1576.302651901328\n",
      "Episode: 20000\n",
      "Total reward = -21875445.327487525\n",
      "Average reward = -1093.7722663743762\n",
      "Episode: 30000\n",
      "Total reward = -27076109.09935655\n",
      "Average reward = -902.5369699785517\n",
      "Episode: 40000\n",
      "Total reward = -31955342.164262284\n",
      "Average reward = -798.8835541065571\n",
      "Episode: 50000\n",
      "Total reward = -36921107.9489731\n",
      "Average reward = -738.422158979462\n",
      "First episode reward: -4874.387506418005\n",
      "Last episode reward: -137.03893604444082\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', max_episode_steps=600)\n",
    "\n",
    "learning_rate = 0.2\n",
    "discount_rate = 0.95\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "action_space_size = 21\n",
    "\"\"\" count of action [-2 : 2] with 0.1 step, 0->19 = [-2 : -0.1], 20 = 0, 21->40 = [0.1 : 2] \"\"\"\n",
    "observation_space_size = [31, 81]\n",
    "\"\"\"\n",
    "First is count of pendulum state [-pi : pi] with 0.1 step, 0->31 = [-pi : -pi/31], 32 = 0, 33->62 = [pi/32 : pi]\n",
    "Second is count of pendulum angular velocity [-8 : 8] with 0.1 step, 0->79 = [-8 : -0.1], 80 = 0, 81->160 = [0.1 : 8]\n",
    "\"\"\"\n",
    "\n",
    "episodes = 50000\n",
    "total_reward_for_episode = list()\n",
    "\n",
    "action_space = np.linspace(-2, 2, num=action_space_size)\n",
    "observation_space = [np.linspace(-np.pi, np.pi, num=observation_space_size[0]),\n",
    "                     np.linspace(-8.0, 8.0, num=observation_space_size[1])]\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=-0, size=(observation_space_size + [action_space_size]))\n",
    "\n",
    "def get_discrete_action(state):\n",
    "    theta = np.arctan2(state[0], state[1])\n",
    "    index_state = np.digitize(theta, observation_space[0]) - 1\n",
    "    index_velocity = np.digitize(state[2], observation_space[1]) - 1\n",
    "    action = q_table[index_state, index_velocity]\n",
    "    return action\n",
    "\n",
    "def get_indexes(state):\n",
    "    theta = np.arctan2(state[0], state[1])\n",
    "    index_state = np.digitize(theta, observation_space[0]) - 1\n",
    "    index_velocity = np.digitize(state[2], observation_space[1]) - 1\n",
    "    return [index_state, index_velocity]\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(get_discrete_action(state))\n",
    "        else:\n",
    "            action = np.random.randint(0, action_space_size)\n",
    "\n",
    "        step = [action_space[action]]\n",
    "\n",
    "        observation, reward, done, truncated, *info = env.step(step)\n",
    "\n",
    "        total_reward += reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Change q table\n",
    "        index_state, index_velocity = get_indexes(state)\n",
    "        new_index_state, new_index_velocity = get_indexes(observation)\n",
    "        new_action = np.argmax(get_discrete_action(observation))\n",
    "\n",
    "        current_q = q_table[index_state, index_velocity, action]\n",
    "        new_step = np.max(q_table[new_index_state, new_index_velocity])\n",
    "        new_q = current_q + learning_rate * (reward + discount_rate * new_step - current_q)\n",
    "        q_table[index_state, index_velocity, action] = new_q\n",
    "        state = observation\n",
    "\n",
    "        # Change epsilon\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        # Truncated\n",
    "        if truncated:\n",
    "            break\n",
    "    \n",
    "    total_reward_for_episode.append(episode_reward)        \n",
    "\n",
    "    if episode % 10000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "        print(\"Total reward = {}\".format(total_reward))\n",
    "        print(\"Average reward = {}\".format(total_reward/episode))\n",
    "\n",
    "print(\"First episode reward: {}\".format(total_reward_for_episode[0]))\n",
    "print(\"Last episode reward: {}\".format(total_reward_for_episode[-1]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T11:21:54.477667100Z",
     "start_time": "2024-04-07T10:14:51.426503400Z"
    }
   },
   "id": "bfd5873a199c985c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test data in tabular form"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d0cea2b37e4649"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Episodes | LR       | DR       | Avg. reward | Total reward | action space size | observation space size | First episode reward | Last episode reward |\n",
    "|-------|---|---|---|---|---|------------------------|----------------------|---------------------|\n",
    "| 10000 | 0.1 | 0.9 | -2921 | -29214268    | 41 | 63, 161                | -3536                |                     |\n",
    "| 20000 | 0.1 | 0.9 | -2631 | -52637180    | 41 | 63, 161                |\n",
    "| 30000 | 0.1 | 0.9 | -2422 | -72678361    | 41 | 63, 161                |\n",
    "| 40000 | 0.1 | 0.9 | -2244 | -89771691    | 41 | 63, 161                |\n",
    "| 50000 | 0.1 | 0.9 | -2091 | -104558145   | 41 | 63, 161                |                      | -607                |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cbeeb40bd1301e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Episodes | LR    | DR    | Avg. reward | Total reward | action space size | observation space size | First episode reward | Last episode reward |\n",
    "|----------|-------|-------|-------------|--------------|-------------------|------------------------|----------------------|---------------------|\n",
    "| 10000    | 0.1   | 0.9   | -2144       | -21443769    | 21                | 31, 81                 | -3693                |                     |\n",
    "| 20000    | 0.1   | 0.9   | -1545       | -30907904    | 21                | 31, 81                 |\n",
    "| 30000    | 0.1   | 0.9   | -1299       | -38977437    | 21                | 31, 81                 |\n",
    "| 40000    | 0.1   | 0.9   | -1156       | -46276821    | 21                | 31, 81                 |\n",
    "| 50000    | 0.1   | 0.9   | -1066       | -53308885    | 21                | 31, 81                 |                      | -906                |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4c18f32082ee072"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Episodes | LR  | DR   | Avg. reward | Total reward | action space size | observation space size | First episode reward | Last episode reward |\n",
    "|-------|-----|------|-------------|--------------|---|------------------------|----------------------|---------------------|\n",
    "| 10000 | 0.2 | 0.95 | -2739       | -27392880    | 41 | 63, 161                | -2823                |                     |\n",
    "| 20000 | 0.2 | 0.95 | -2305       | -46105374    | 41 | 63, 161                |\n",
    "| 30000 | 0.2 | 0.95 | -2001       | -60039006    | 41 | 63, 161                |\n",
    "| 40000 | 0.2 | 0.95 | -1769       | -70791353    | 41 | 63, 161                |\n",
    "| 50000 | 0.2 | 0.95 | -1586       | -79316870    | 41 | 63, 161                |                      | -869                |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2134caf179ae8b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Episodes | LR  | DR   | Avg. reward | Total reward | action space size | observation space size | First episode reward | Last episode reward |\n",
    "|-------|-----|------|-------------|--------------|-------------------|------------------------|----------------------|---------------------|\n",
    "| 10000 | 0.2 | 0.95 | -1576       | -15763026    | 21                | 31, 81                | -4874                |                     |\n",
    "| 20000 | 0.2 | 0.95 | -1093       | -21875445    | 21                | 31, 81                |\n",
    "| 30000 | 0.2 | 0.95 | -902        | -27076109    | 21                | 31, 81                |\n",
    "| 40000 | 0.2 | 0.95 | -798        | -31955342    | 21                | 31, 81                |\n",
    "| 50000 | 0.2 | 0.95 | -738        | -36921107    | 21                | 31, 81                |                      | -137                |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7bd1e546729218c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3dd17e9570f36d09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
