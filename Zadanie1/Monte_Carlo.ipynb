{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pendulum\n",
    "## The notebook describes a solution of the problem of controlling a pendulum to be raised to a vertical position and held in this state using a on-policy first-visit Monte Carlo control method "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5e3293477090e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Workspace\n",
    "Workspace is divided into 4 values, the cartesian coordinate system x and y, theta angle in radians and tau the torque of the pendulum\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac702c5c4a8ed5f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Action Space\n",
    "Force applied to the pendulum in the range of continuous values from -2.0 to 2.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d24fcd84c78ecb8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation Space\n",
    "Observation Space contains 3 values, the x and y coordinates, and the angular velocity of the pendulum.\n",
    "x = cos(theta) continuous value from -1.0 to 1.0\n",
    "y = sin(theta) continuous value from -1.0 to 1.0\n",
    "Angular velocity is continuous value from -8.0 to 8.0\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a667d1a4050729a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I've made those changes:\n",
    "Converted observation space from 3 values to 2, by obtaining theta angle from its cosine and sine using the x and n values I know"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd87128deed95fb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I used the built-in award function which is described as:\n",
    "### r = -(theta^2^ + 0.1 * theta_dt^2^ + 0.001 * torque^2^)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5affd3ffe780a5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## My solution is written based on this pseudocode\n",
    "![Pseudo code](./mc_onpolicy.jpg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f031e990a302e45"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "abfb7ae8bbde4415"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-07T13:09:43.118821300Z",
     "start_time": "2024-04-07T13:09:35.601132500Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "PRELOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# env = gym.make('Pendulum-v1', render_mode=\"human\", max_episode_steps=600); env.metadata['render_fps'] = 60\n",
    "env = gym.make('Pendulum-v1', max_episode_steps=600)\n",
    "\n",
    "count_of_episodes = 500000\n",
    "discount_rate = 0.95\n",
    "\n",
    "if not PRELOAD:\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.9999\n",
    "else:\n",
    "    epsilon = 0.15\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9999\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "action_space_size = 41\n",
    "\"\"\" count of action [-2 : 2] with 0.1 step, 0->19 = [-2 : -0.1], 20 = 0, 21->40 = [0.1 : 2] \"\"\"\n",
    "observation_space_size = [63, 161]\n",
    "\"\"\"\n",
    "First is count of pendulum state [-pi : pi] with 0.1 step, 0->31 = [-pi : -pi/32], 32 = 0, 33->62 = [pi/32 : pi]\n",
    "Second is count of pendulum angular velocity [-8 : 8] with 0.1 step, 0->79 = [-8 : -0.1], 80 = 0, 81->160 = [0.1 : 8]\n",
    "\"\"\"\n",
    "\n",
    "total_reward_for_episode = list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T20:10:47.948234900Z",
     "start_time": "2024-04-07T20:10:47.914236800Z"
    }
   },
   "id": "ab9d8e79eea619af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "action_space = np.linspace(-2, 2, num=action_space_size)\n",
    "observation_space = [np.linspace(-np.pi, np.pi, num=observation_space_size[0]),\n",
    "                     np.linspace(-8.0, 8.0, num=observation_space_size[1])]\n",
    "if not PRELOAD:\n",
    "    q_table = np.random.uniform(low=-1, high=1, size=(observation_space_size + [action_space_size]))\n",
    "    # q_table = np.zeros(observation_space_size + [action_space_size])\n",
    "    returns_table = np.zeros(observation_space_size + [action_space_size])\n",
    "    returns_table_count = np.zeros(observation_space_size + [action_space_size])\n",
    "    policy = np.ones((observation_space_size + [action_space_size])) / action_space_size\n",
    "else:\n",
    "    q_table = np.load(\"D:/Programming/MachineLearning2/Zadanie1/MC_q_table_pendulum.npy\")\n",
    "    policy = np.load(\"D:/Programming/MachineLearning2/Zadanie1/policy_table.npy\")\n",
    "    returns_tables = np.load(\"D:/Programming/MachineLearning2/Zadanie1/returns_tables.npy\")\n",
    "    returns_table = returns_tables[0]\n",
    "    returns_table_count = returns_tables[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b9b9168b31b8b85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_indexes(state):\n",
    "    theta = np.arctan2(state[0], state[1])\n",
    "    index_state = np.digitize(theta, observation_space[0]) - 1\n",
    "    index_velocity = np.digitize(state[2], observation_space[1]) - 1\n",
    "    return index_state, index_velocity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56ab2fce1397758c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def choose_action(index_state, index_velocity):\n",
    "    action_probabilities = policy[index_state, index_velocity]\n",
    "    action = np.random.choice(np.arange(action_space_size), p=action_probabilities)\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dadac92bb954d581"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for episode in range(1, count_of_episodes+1):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    episode_history = list()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        index_state, index_velocity = get_indexes(state)\n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "            # action = np.argmax(get_discrete_action(state))\n",
    "            action = choose_action(index_state, index_velocity)\n",
    "        else:\n",
    "            action = np.random.randint(0, action_space_size)\n",
    "\n",
    "        step = [action_space[action]]\n",
    "        observation, reward, done, truncated, *info = env.step(step)\n",
    "\n",
    "        total_reward += reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # new_index_state, new_index_velocity = get_indexes(observation)\n",
    "\n",
    "        episode_history.append(((index_state, index_velocity), action, reward))\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        if truncated:\n",
    "            break\n",
    "\n",
    "    G = 0\n",
    "    for i in reversed(range(0, len(episode_history))):\n",
    "        \n",
    "        this_episode = episode_history[i]\n",
    "        indexes, action, reward = this_episode\n",
    "        G = discount_rate * G + reward\n",
    "\n",
    "        if (indexes, action) not in episode_history[:-1]:\n",
    "            # index_state, index_velocity = indexes\n",
    "            returns_table[indexes[0], indexes[1], action] += G\n",
    "            returns_table_count[indexes[0], indexes[1], action] += 1\n",
    "            q_table[indexes[0], indexes[1], action] = \\\n",
    "                returns_table[indexes[0], indexes[1], action] /\\\n",
    "                returns_table_count[indexes[0], indexes[1], action]\n",
    "\n",
    "            best_action_index = np.argmax(q_table[indexes[0], indexes[1]])\n",
    "            for a in range(action_space_size):\n",
    "                if a == best_action_index:\n",
    "                    policy[indexes[0], indexes[1], a] = 1 - epsilon + (epsilon / action_space_size)\n",
    "                else:\n",
    "                    policy[indexes[0], indexes[1], a] = epsilon / action_space_size\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "        # print(\"Total reward = {}\".format(total_reward))\n",
    "        print(\"Average reward = {}\\n\".format(total_reward/episode))\n",
    "\n",
    "        returns_tables = [returns_table, returns_table_count]\n",
    "        np.save(\"D:/Programming/MachineLearning2/Zadanie1/MC_q_table_pendulum\", q_table)\n",
    "        np.save(\"D:/Programming/MachineLearning2/Zadanie1/returns_tables\", returns_tables)\n",
    "        np.save(\"D:/Programming/MachineLearning2/Zadanie1/policy_table\", policy)\n",
    "\n",
    "    total_reward_for_episode.append(episode_reward)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f60839927f89687b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(total_reward_for_episode)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "654308f88605456d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "returns_tables = [returns_table, returns_table_count]\n",
    "np.save(\"D:/Programming/MachineLearning2/Zadanie1/MC_q_table_pendulum\", q_table)\n",
    "np.save(\"D:/Programming/MachineLearning2/Zadanie1/returns_tables\", returns_tables)\n",
    "np.save(\"D:/Programming/MachineLearning2/Zadanie1/policy_table\", policy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8427bcdf15b0f8bf"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fee56e725a24f3be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6734dea23af16b6"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6550bf04109e14d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "Total reward = -37263544.787386894\n",
      "Average reward = -3726.3544787386895\n",
      "Episode: 20000\n",
      "Total reward = -73316519.6250489\n",
      "Average reward = -3665.825981252445\n",
      "Episode: 30000\n",
      "Total reward = -109187917.38633704\n",
      "Average reward = -3639.5972462112345\n",
      "Episode: 40000\n",
      "Total reward = -144906881.40441257\n",
      "Average reward = -3622.672035110314\n",
      "Episode: 50000\n",
      "Total reward = -180479409.51723495\n",
      "Average reward = -3609.588190344699\n",
      "\n",
      "First episode reward: -4936.852030573195\n",
      "Last episode reward: -4812.688742901135\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', max_episode_steps=600)\n",
    "\n",
    "count_of_episodes = 50000\n",
    "discount_rate = 0.95\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.9999\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "action_space_size = 21\n",
    "\"\"\" count of action [-2 : 2] with 0.1 step, 0->19 = [-2 : -0.1], 20 = 0, 21->40 = [0.1 : 2] \"\"\"\n",
    "observation_space_size = [31, 81]\n",
    "\"\"\"\n",
    "First is count of pendulum state [-pi : pi] with 0.1 step, 0->31 = [-pi : -pi/32], 32 = 0, 33->62 = [pi/32 : pi]\n",
    "Second is count of pendulum angular velocity [-8 : 8] with 0.1 step, 0->79 = [-8 : -0.1], 80 = 0, 81->160 = [0.1 : 8]\n",
    "\"\"\"\n",
    "\n",
    "total_reward_for_episode = list()\n",
    "action_space = np.linspace(-2, 2, num=action_space_size)\n",
    "observation_space = [np.linspace(-np.pi, np.pi, num=observation_space_size[0]),\n",
    "                     np.linspace(-8.0, 8.0, num=observation_space_size[1])]\n",
    "\n",
    "q_table = np.random.uniform(low=-1, high=1, size=(observation_space_size + [action_space_size]))\n",
    "# q_table = np.zeros(observation_space_size + [action_space_size])\n",
    "returns_table = np.zeros(observation_space_size + [action_space_size])\n",
    "returns_table_count = np.zeros(observation_space_size + [action_space_size])\n",
    "policy = np.ones((observation_space_size + [action_space_size])) / action_space_size\n",
    "\n",
    "def get_indexes(state):\n",
    "    theta = np.arctan2(state[0], state[1])\n",
    "    index_state = np.digitize(theta, observation_space[0]) - 1\n",
    "    index_velocity = np.digitize(state[2], observation_space[1]) - 1\n",
    "    return index_state, index_velocity\n",
    "\n",
    "\n",
    "def choose_action(index_state, index_velocity):\n",
    "    action_probabilities = policy[index_state, index_velocity]\n",
    "    action = np.random.choice(np.arange(action_space_size), p=action_probabilities)\n",
    "    return action\n",
    "\n",
    "\n",
    "for episode in range(1, count_of_episodes+1):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    episode_history = list()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        index_state, index_velocity = get_indexes(state)\n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "            # action = np.argmax(get_discrete_action(state))\n",
    "            action = choose_action(index_state, index_velocity)\n",
    "        else:\n",
    "            action = np.random.randint(0, action_space_size)\n",
    "\n",
    "        step = [action_space[action]]\n",
    "        observation, reward, done, truncated, *info = env.step(step)\n",
    "\n",
    "        total_reward += reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # new_index_state, new_index_velocity = get_indexes(observation)\n",
    "\n",
    "        episode_history.append(((index_state, index_velocity), action, reward))\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        if truncated:\n",
    "            break\n",
    "\n",
    "    G = 0\n",
    "    for i in reversed(range(0, len(episode_history))):\n",
    "        # reward = episode_history[i][-1]\n",
    "\n",
    "        this_episode = episode_history[i]\n",
    "        indexes, action, reward = this_episode\n",
    "        G = discount_rate * G + reward\n",
    "\n",
    "        if (indexes, action) not in episode_history[:-1]:\n",
    "            # index_state, index_velocity = indexes\n",
    "            returns_table[indexes[0], indexes[1], action] += G\n",
    "            returns_table_count[indexes[0], indexes[1], action] += 1\n",
    "            q_table[indexes[0], indexes[1], action] = \\\n",
    "                returns_table[indexes[0], indexes[1], action] /\\\n",
    "                returns_table_count[indexes[0], indexes[1], action]\n",
    "\n",
    "            best_action_index = np.argmax(q_table[indexes[0], indexes[1]])\n",
    "            for a in range(action_space_size):\n",
    "                if a == best_action_index:\n",
    "                    policy[indexes[0], indexes[1], a] = 1 - epsilon + (epsilon / action_space_size)\n",
    "                else:\n",
    "                    policy[indexes[0], indexes[1], a] = epsilon / action_space_size\n",
    "\n",
    "    total_reward_for_episode.append(episode_reward)\n",
    "    \n",
    "    if episode % 10000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "        print(\"Total reward = {}\".format(total_reward))\n",
    "        print(\"Average reward = {}\\n\".format(total_reward/episode))\n",
    "    \n",
    "    \n",
    "print(\"First episode reward: {}\".format(total_reward_for_episode[0]))\n",
    "print(\"Last episode reward: {}\".format(total_reward_for_episode[-1]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T23:09:50.464265300Z",
     "start_time": "2024-04-07T21:38:51.923796900Z"
    }
   },
   "id": "68272ac3235b72b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test data in tabular form"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9b6f30b89a8ece2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Episodes | DR   | Avg. reward | Total reward    | action space size | observation space size | First episode reward | Last episode reward |\n",
    "|-------|---|------------|-----------------|---|------------------------|---------------------|---------------------|\n",
    "| 10000 | 0.9 | -3914      | -39145430       | 41 | 63, 161                | -3452               |                     |\n",
    "| 20000 | 0.9 | -3823      | -76474999       | 41 | 63, 161                |\n",
    "| 30000 | 0.9 | -3759      | -112790082      | 41 | 63, 161                |\n",
    "| 40000 | 0.9 | -3721      | -148847811      | 41 | 63, 161                |\n",
    "| 50000 | 0.9 | -3698      | -184915135      | 41 | 63, 161                |                     | -3491               |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e15abb4027483e71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Episodes | DR   | Avg. reward | Total reward    | action space size | observation space size | First episode reward | Last episode reward |\n",
    "|-------|---|-------------|-----------------|-------------------|------------------------|----------------------|---------------------|\n",
    "| 10000 | 0.9 | -3712       | -37122246       | 21                | 31, 81                 | -3029                |                     |\n",
    "| 20000 | 0.9 | -3648       | -72965356       | 21                | 31, 81                 |\n",
    "| 30000 | 0.9 | -3625       | -108773386      | 21                | 31, 81                 |\n",
    "| 40000 | 0.9 | -3611       | -144442189      | 21                | 31, 81                 |\n",
    "| 50000 | 0.9 | -3602       | -180112652      | 21                | 31, 81                 |                      | -3111                |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6a55237ce5e04f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Episodes | DR   | Avg. reward | Total reward | action space size | observation space size | First episode reward | Last episode reward |\n",
    "|-------|------|-------------|-------------|-------------------|------------------------|----------------------|---------------------|\n",
    "| 10000 | 0.95 | -3906       | -39069204   | 41                | 63, 161                | -4275                |                     |\n",
    "| 20000 | 0.95 | -3819       | -76394531   | 41                | 63, 161                |\n",
    "| 30000 | 0.95 | -3760       | -112807625  | 41                | 63, 161                |\n",
    "| 40000 | 0.95 | -3725       | -149016157  | 41                | 63, 161                |\n",
    "| 50000 | 0.95 | -3698       | -184924282  | 41                | 63, 161                |                      | -4006               |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c0758002bcc50b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Episodes | DR   | Avg. reward | Total reward | action space size | observation space size | First episode reward | Last episode reward |\n",
    "|-------|------|-------------|-------------|-------------------|------------------------|----------------------|---------------------|\n",
    "| 10000 | 0.95 | -3726       | -37263544   | 21                | 31, 81                 | -4936                |                     |\n",
    "| 20000 | 0.95 | -3665       | -73316519   | 21                | 31, 81                 |\n",
    "| 30000 | 0.95 | -3639       | -109187917  | 21                | 31, 81                 |\n",
    "| 40000 | 0.95 | -3622       | -144906881  | 21                | 31, 81                 |\n",
    "| 50000 | 0.95 | -3609       | -180479409  | 21                | 31, 81                 |                      | -4812               |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f39c090964ad528b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1e008f3400c37c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "40041f3ada2ef83e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
